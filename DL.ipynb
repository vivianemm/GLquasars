{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import MinMaxInterval\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a FITS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/vivianemm/BLF/Images/EUC_J/imageEUC_J-200001.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU      29   (66, 66)   float32   \n"
     ]
    }
   ],
   "source": [
    "#img_path = '/home/viviane/BLF_training/Images/EUC_J/imageEUC_J-200001.fits'\n",
    "img_path = '/home/vivianemm/BLF/Images/EUC_J/imageEUC_J-200001.fits'\n",
    "fits.info(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.76715930e-12  5.15664941e-12  1.09206437e-11 ... -5.59254899e-12\n",
      "  -3.11408560e-12  4.89244192e-12]\n",
      " [-1.53540358e-11  1.29798325e-11 -2.45361175e-12 ...  7.96089583e-12\n",
      "  -1.83813902e-11 -2.78579763e-12]\n",
      " [ 1.88017896e-12  9.41518478e-12  6.43359443e-13 ...  9.76120226e-13\n",
      "   5.17626610e-12 -4.57186676e-12]\n",
      " ...\n",
      " [-6.50529934e-12  1.61818284e-11  7.76066971e-12 ... -5.27057131e-12\n",
      "   5.44669128e-12  9.69319460e-13]\n",
      " [ 1.20240528e-11 -1.82952351e-12  2.04764972e-11 ...  1.14301635e-12\n",
      "   1.23559182e-12 -1.11187286e-12]\n",
      " [ 2.80688558e-12 -8.62631754e-12  4.48427536e-12 ...  4.49563910e-12\n",
      "   4.55615884e-12  3.62280019e-12]]\n"
     ]
    }
   ],
   "source": [
    "### using fits.getdata() --> np array\n",
    "img_data = fits.getdata(img_path, ext=0)\n",
    "print(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f2eb300dd30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(img_data, cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIMPLE  =                    T / file does conform to FITS standard             \n",
       "BITPIX  =                  -32 / number of bits per data pixel                  \n",
       "NAXIS   =                    2 / number of data axes                            \n",
       "NAXIS1  =                   66 / length of data axis 1                          \n",
       "NAXIS2  =                   66 / length of data axis 2                          \n",
       "EXTEND  =                    T / FITS dataset may contain extensions            \n",
       "COMMENT   FITS (Flexible Image Transport System) format is defined in 'Astronomy\n",
       "COMMENT   and Astrophysics', volume 376, page 359; bibcode: 2001A&A...376..359H \n",
       "WCSAXES =                    2 / number of World Coordinate System axes         \n",
       "CRPIX1  =                 33.5 / x-coordinate of reference pixel                \n",
       "CRPIX2  =                 33.5 / y-coordinate of reference pixel                \n",
       "CRVAL1  =                   0. / first axis value at reference pixel            \n",
       "CRVAL2  =                   0. / second axis value at reference pixel           \n",
       "CTYPE1  = 'RA---TAN'           / the coordinate type for the first axis         \n",
       "CTYPE2  = 'DEC--TAN'           / the coordinate type for the second axis        \n",
       "CUNIT1  = 'deg     '           / the coordinate unit for the first axis         \n",
       "CUNIT2  = 'deg     '           / the coordinate unit for the second axis        \n",
       "CDELT1  = -8.33333260176475E-05 / partial of first axis coordinate w.r.t. x     \n",
       "CDELT2  = 8.33333260176475E-05 / partial of second axis coordinate w.r.t. y     \n",
       "CROTA2  =                   0.                                                  \n",
       "CD1_1   = -8.33333260176475E-05 / partial of first axis coordinate w.r.t. x     \n",
       "CD1_2   =                   0. / partial of first axis coordinate w.r.t. y      \n",
       "CD2_1   =                   0. / partial of second axis coordinate w.r.t. x     \n",
       "CD2_2   = 8.33333260176475E-05 / partial of second axis coordinate w.r.t. y     \n",
       "NX      =                   66                                                  \n",
       "NY      =                   66                                                  \n",
       "HIERARCH range x = 9.59931110173784E-05 / radians                               \n",
       "RA      = 3.46759270735022E-07 / radians                                        \n",
       "DEC     = 1.38897332534521E-07 / radians                                        "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or using fits.open()  --> HDU object\n",
    "hdul = fits.open(img_path)\n",
    "img_data2 = hdul[0].data  # nparray\n",
    "hdul[0].header\n",
    "#hdul.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import cv2\n",
    "import os\n",
    "def get_data(IMAGE_DIMS):\n",
    "    \n",
    "    blf = pd.read_csv('Database/blf_complete.csv', comment=\"#\")\n",
    "    blf = blf.head(10000)\n",
    "\n",
    "    imgs = []\n",
    "    for path in blf['img_path_H']:\n",
    "        #print(path)\n",
    "        #hdul = fits.open(path)\n",
    "        #img_data = hdul[0].data  # nparray\n",
    "        #hdul.close()\n",
    "        image = cv2.imread(path)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "            Data1=image\n",
    "            Data1[:,:,0]=image[:,:,2]\n",
    "            Data1[:,:,2]=image[:,:,0]\n",
    "            image=Data1\n",
    "            image = img_to_array(image)            \n",
    "        \n",
    "        #data.append(image)\n",
    "\n",
    "        #img_data = fits.getdata(path, ext=0)  # np array\n",
    "        #gc.collect()\n",
    "        \n",
    "        #img_data *= 1/img_data.max()\n",
    "        imgs.append(image)\n",
    "    imgs = np.array(imgs)\n",
    "    imgs = imgs / 255.0\n",
    "\n",
    "    labels = np.array(blf['n_sources'])\n",
    "    \n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = get_data((66,66,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 66, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_list =[]\n",
    "for i in range(0, len(imgs)):\n",
    "    if imgs[i] is None:\n",
    "        i_list.append(i)\n",
    "        \n",
    "labels = np.delete(labels, i_list, 0)\n",
    "imgs = np.delete(imgs, i_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 66, 66, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[]\n",
    "for i in range(0,imgs.shape[0]):\n",
    "    l.append(imgs[i])\n",
    "l=np.array(l)\n",
    "l.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"imagens.npy\", imgs)\n",
    "np.save(\"labels.npy\", labels)\n",
    "# imgs = np.load(\"imagens.npy\")\n",
    "# labels = np.load(\"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 66, 66, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, imgs_test, labels, labels_test = train_test_split(imgs, labels, test_size=0.1, random_state=42)\n",
    "imgs, imgs_val, labels, labels_val = train_test_split(imgs, labels, test_size=0.30, random_state=42)\n",
    "#imgs = imgs.reshape(imgs.shape + (1,) )\n",
    "#imgs_test = imgs_test.reshape(imgs_test.shape + (1,))\n",
    "#imgs_val = imgs_val.reshape(imgs_val.shape + (1,))\n",
    "imgs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), input_shape = (66,66,3), activation = \"relu\"))\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = \"relu\"))\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = \"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(25, activation = \"relu\"))\n",
    "    model.add(Dense(15, activation = \"relu\"))\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vivianemm/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 6300 samples, validate on 2700 samples\n",
      "Epoch 1/30\n",
      "6300/6300 [==============================] - 42s 7ms/step - loss: 0.4835 - accuracy: 0.8903 - val_loss: 0.3343 - val_accuracy: 0.9048\n",
      "Epoch 2/30\n",
      "6300/6300 [==============================] - 40s 6ms/step - loss: 0.3050 - accuracy: 0.8957 - val_loss: 0.3869 - val_accuracy: 0.9015\n",
      "Epoch 3/30\n",
      "6300/6300 [==============================] - 40s 6ms/step - loss: 0.2198 - accuracy: 0.9186 - val_loss: 0.5025 - val_accuracy: 0.8970\n",
      "Epoch 4/30\n",
      "6300/6300 [==============================] - 40s 6ms/step - loss: 0.1359 - accuracy: 0.9492 - val_loss: 0.6961 - val_accuracy: 0.8737\n",
      "Epoch 5/30\n",
      "6300/6300 [==============================] - 41s 6ms/step - loss: 0.0912 - accuracy: 0.9660 - val_loss: 0.9540 - val_accuracy: 0.8593\n",
      "Epoch 6/30\n",
      "6300/6300 [==============================] - 42s 7ms/step - loss: 0.0647 - accuracy: 0.9773 - val_loss: 0.8588 - val_accuracy: 0.8341\n",
      "Epoch 7/30\n",
      "6300/6300 [==============================] - 40s 6ms/step - loss: 0.0507 - accuracy: 0.9822 - val_loss: 1.3555 - val_accuracy: 0.8519\n",
      "Epoch 8/30\n",
      "6300/6300 [==============================] - 41s 6ms/step - loss: 0.0334 - accuracy: 0.9879 - val_loss: 1.4071 - val_accuracy: 0.8678\n",
      "Epoch 9/30\n",
      "6300/6300 [==============================] - 43s 7ms/step - loss: 0.0331 - accuracy: 0.9892 - val_loss: 1.6037 - val_accuracy: 0.8585\n",
      "Epoch 10/30\n",
      "6300/6300 [==============================] - 43s 7ms/step - loss: 0.0177 - accuracy: 0.9933 - val_loss: 1.7309 - val_accuracy: 0.8515\n",
      "Epoch 11/30\n",
      "6300/6300 [==============================] - 43s 7ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 2.1814 - val_accuracy: 0.8463\n",
      "Epoch 12/30\n",
      "6300/6300 [==============================] - 43s 7ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 1.4431 - val_accuracy: 0.8507\n",
      "Epoch 13/30\n",
      "6300/6300 [==============================] - 41s 7ms/step - loss: 0.0452 - accuracy: 0.9857 - val_loss: 1.6211 - val_accuracy: 0.8396\n",
      "Epoch 14/30\n",
      "6300/6300 [==============================] - 42s 7ms/step - loss: 0.0201 - accuracy: 0.9940 - val_loss: 1.8138 - val_accuracy: 0.8248\n",
      "Epoch 15/30\n",
      "6300/6300 [==============================] - 43s 7ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 2.3053 - val_accuracy: 0.8393\n",
      "Epoch 16/30\n",
      "6300/6300 [==============================] - 42s 7ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 2.3499 - val_accuracy: 0.8137\n",
      "Epoch 17/30\n",
      "6300/6300 [==============================] - 42s 7ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 2.4590 - val_accuracy: 0.8444\n",
      "Epoch 18/30\n",
      "6272/6300 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-a2bcb43538c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                          verbose=0)\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = modelo.fit(imgs,labels, epochs=30, validation_data = (imgs_val, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 30), history.history[\"loss\"], label = \"Erro de trainamento\")\n",
    "plt.plot(np.arange(0, 30), history.history[\"val_loss\"], label = \"Erro de validação\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(np.arange(0, 30), history.history[\"acc\"], label = \"Erro de trainamento\")\n",
    "#plt.plot(np.arange(0, 30), history.history[\"val_acc\"], label = \"Erro de validação\")\n",
    "#plt.xlabel(\"Epoch\")\n",
    "#plt.legend(loc=\"upper left\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_alt():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(55, kernel_size = (3,3), input_shape = (66,66,1), activation = \"relu\"))\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = \"relu\"))\n",
    "    model.add(Conv2D(52, kernel_size = (3,3), activation = \"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(25, activation = \"relu\"))\n",
    "    model.add(Dense(15, activation = \"relu\"))\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_alt = model_alt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_alt.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "history_alt = modelo_alt.fit(imgs,labels, epochs=30, validation_data = (imgs_val, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 30), history_alt.history[\"loss\"], label = \"Loss modelo alt\")\n",
    "plt.plot(np.arange(0, 30), history.history[\"loss\"], label = \"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 30), history_alt.history[\"val_loss\"], label = \"val_loss alt\")\n",
    "plt.plot(np.arange(0, 30), history.history[\"val_loss\"], label = \"Val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('Database/blf_complete.csv', comment=\"#\")\n",
    "a.set_index(a['ID'])\n",
    "a #['img_path_H'] #[213913]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
